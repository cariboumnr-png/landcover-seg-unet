# pylint: disable=missing-class-docstring
# pylint: disable=missing-function-docstring
# pylint: disable=too-few-public-methods
'''
Trainer building facing components protocols.
'''

# standard imports
from __future__ import annotations
import typing
# local imports
import training.trainer

if typing.TYPE_CHECKING:
    import torch

# -----------------------------trainer components-----------------------------
# components collection
@typing.runtime_checkable
class TrainerComponentsLike(typing.Protocol):
    model: MultiheadModelLike
    dataloaders: DataLoadersLike
    headspecs: HeadSpecsLike
    headlosses: HeadLossesLike
    headmetrics: HeadMetricsLike
    optimization: OptimizationLike
    callbacks: CallBacksLike

# trainer component - trainable and checkpointable model
@typing.runtime_checkable
class MultiheadModelLike(typing.Protocol):
    def __call__(self, x: 'torch.Tensor', **kwargs) -> typing.Mapping[str, 'torch.Tensor']: ...
    def forward(self, x: 'torch.Tensor', **kwargs) -> typing.Mapping[str, 'torch.Tensor']: ...
    def parameters(self) -> typing.Iterable['torch.nn.Parameter']: ...
    def to(self: typing.Self, device: 'torch.device | str') -> typing.Self: ...
    def train(self: typing.Self, mode: bool = True) -> typing.Self: ...
    def eval(self: typing.Self) -> typing.Self: ...
    def set_active_heads(self, active_heads: list[str] | None) -> None: ...
    def set_frozen_heads(self, frozen_heads: list[str] | None) -> None: ...
    def reset_heads(self) -> None: ...
    def state_dict(self) -> typing.Mapping[str, 'torch.Tensor']: ...
    def load_state_dict(self, state_dict: typing.Mapping[str, typing.Any]) -> typing.Any: ...

# trainer component - dataloaders
@typing.runtime_checkable
class DataLoadersLike(typing.Protocol):
    train: 'torch.utils.data.DataLoader'
    val: 'torch.utils.data.DataLoader'
    test: 'torch.utils.data.DataLoader | None'
    @property
    def meta(self) -> _LoaderMetaLike: ...

class _LoaderMetaLike(typing.Protocol):
    batch_size: int
    patch_per_blk: int
    test_blks_grid: tuple[int, int]

# trainer component - head specs (wrapper and individual spec)
@typing.runtime_checkable
class HeadSpecsLike(typing.Protocol):
    def __getitem__(self, key: str) -> SpecLike: ...
    def __len__(self) -> int: ...
    def as_dict(self) -> dict[str, SpecLike]: ...

@typing.runtime_checkable
class SpecLike(typing.Protocol):
    name: str
    count: list[int]
    loss_alpha: list[float]
    parent_head: str | None
    parent_cls: int | None
    weight: float
    exclude_cls: tuple[int, ...]
    def set_exclude(self, indices: tuple[int, ...]) -> None: ...
    @property
    def num_classes(self) -> int: ...

# trainer component - loss modules (wrapper and individual loss module)
@typing.runtime_checkable
class HeadLossesLike(typing.Protocol):
    def __getitem__(self, key: str) -> CompositeLossLike: ...
    def __len__(self) -> int: ...
    def as_dict(self) -> dict[str, CompositeLossLike]: ...

@typing.runtime_checkable
class CompositeLossLike(typing.Protocol):
    ignore_index: int
    def forward(self, p: 'torch.Tensor', t: 'torch.Tensor', **kwargs) -> 'torch.Tensor': ...

# trainer component - metric modules (wrapper and individual metric module)
@typing.runtime_checkable
class HeadMetricsLike(typing.Protocol):
    def __getitem__(self, key: str) -> MetricLike: ...
    def __len__(self) -> int: ...
    def as_dict(self) -> dict[str, MetricLike]: ...

@typing.runtime_checkable
class MetricLike(typing.Protocol):
    exclude_class_1b: tuple[int, ...] | None
    def update(self, p0: 'torch.Tensor', t1: 'torch.Tensor', **kwargs) -> None: ...
    def compute(self) -> None: ...
    def reset(self, device: str) -> None: ...
    @property
    def metrics_dict(self) -> dict[str, typing.Any]: ...
    @property
    def metrics_text(self) -> list[str]: ...

# trainer component - optimization
@typing.runtime_checkable
class OptimizationLike(typing.Protocol):
    optimizer: torch.optim.Optimizer
    scheduler: torch.optim.lr_scheduler.LRScheduler | None

# trainer component - callbacks
@typing.runtime_checkable
class CallBacksLike(typing.Protocol):
    progress: ProgressCallbackLike
    train: TrainCallbackLike
    validate: ValCallbackLike
    logging: LoggingCallbackLike
    def __iter__(self) -> typing.Iterator[CallBackBaseLike]: ...

@typing.runtime_checkable
class CallBackBaseLike(typing.Protocol):
    def setup(self, trainer: training.trainer.MultiHeadTrainer) -> None: ...

@typing.runtime_checkable
class ProgressCallbackLike(typing.Protocol):
    def on_train_epoch_begin(self, epoch: int) -> None: ...
    def on_train_batch_end(self) -> None: ...
    def on_train_epoch_end(self) -> None: ...
    def on_validation_begin(self) -> None: ...
    def on_validation_end(self) -> None: ...

@typing.runtime_checkable
class TrainCallbackLike(typing.Protocol):
    def on_train_epoch_begin(self, epoch: int) -> None: ...
    def on_train_batch_begin(self, bidx: int, batch: tuple) -> None: ...
    def on_train_batch_forward(self) -> None: ...
    def on_train_batch_compute_loss(self) -> None: ...
    def on_train_backward(self) -> None: ...
    def on_train_before_optimizer_step(self) -> None: ...
    def on_train_optimizer_step(self) -> None: ...
    def on_train_batch_end(self) -> None: ...
    def on_train_epoch_end(self) -> None: ...

@typing.runtime_checkable
class ValCallbackLike(typing.Protocol):
    def on_validation_begin(self) -> None: ...
    def on_validation_batch_begin(self, bidx: int, batch: tuple) -> None: ...
    def on_validation_batch_forward(self) -> None: ...
    def on_validation_batch_end(self) -> None: ...
    def on_validation_end(self) -> None: ...

@typing.runtime_checkable
class InferCallbackLike(typing.Protocol):
    def on_inference_begin(self) -> None: ...
    def on_inference_batch_begin(self, bidx: int, batch: tuple) -> None: ...
    def on_inference_batch_forward(self) -> None: ...
    def on_inference_batch_end(self) -> None: ...
    def on_inference_end(self) -> None: ...

@typing.runtime_checkable
class LoggingCallbackLike(typing.Protocol):
    def on_train_epoch_begin(self, epoch: int) -> None: ...
    def on_train_batch_begin(self, bidx: int, batch: tuple) -> None: ...
    def on_train_batch_end(self) -> None: ...
    def on_train_epoch_end(self) -> None: ...
    def on_validation_begin(self) -> None: ...
    def on_validation_batch_begin(self, bidx: int, batch: tuple) -> None: ...
    def on_validation_end(self) -> None: ...
